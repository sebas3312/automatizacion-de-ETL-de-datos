import pandas as pd
from sqlalchemy import create_engine
from airflow import DAG
from airflow.operators.python_operator import PythonOperator
from datetime import datetime, timedelta

# Configuración de la conexión a la base de datos
db_engine = create_engine('postgresql://username:password@host:port/database')

# Función para extraer datos desde archivos CSV
def extract_data_from_csv():
    data = pd.read_csv('input_data.csv')
    return data

# Función para transformar y limpiar los datos
def transform_data(data):
    data = data.dropna()
    data['new_column'] = data['existing_column'].apply(lambda x: x.upper())
    return data

# Función para cargar los datos transformados a la base de datos
def load_data_to_db(data):
    data.to_sql('table_name', db_engine, if_exists='append', index=False)

# Definición del flujo de trabajo de ETL
def etl_pipeline():
    data = extract_data_from_csv()
    transformed_data = transform_data(data)
    load_data_to_db(transformed_data)

# Definición del DAG (Directed Acyclic Graph) de Airflow
default_args = {
    'owner': 'your_name',
    'depends_on_past': False,
    'start_date': datetime(2023, 4, 9),
    'email': ['you@example.com'],
    'email_on_failure': False,
    'email_on_retry': False,
    'retries': 1,
    'retry_delay': timedelta(minutes=5)
}

dag = DAG(
    'etl_pipeline',
    default_args=default_args,
    description='Automatización de Procesos de Extracción, Transformación y Carga (ETL) de Datos',
    schedule_interval=timedelta(days=1)
)

# Definición de las tareas del DAG
extract_task = PythonOperator(
    task_id='extract_data',
    python_callable=extract_data_from_csv,
    dag=dag
)

transform_task = PythonOperator(
    task_id='transform_data',
    python_callable=transform_data,
    op_args=[extract_task.output],
    dag=dag
)

load_task = PythonOperator(
    task_id='load_data',
    python_callable=load_data_to_db,
    op_args=[transform_task.output],
    dag=dag
)

# Definición del flujo de tareas
extract_task >> transform_task >> load_task
